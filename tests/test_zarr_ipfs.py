# import time

# import numpy as np
# import pandas as pd
# import xarray as xr
# import pytest
# import zarr
# import zarr.core.buffer

# from py_hamt import HAMT, KuboCAS

# from py_hamt.zarr_hamt_store import ZarrHAMTStore
# from testing_utils import (
#     create_ipfs,
# )  # noqa pytest detects by name but this shows up as unused import error


# @pytest.fixture(scope="module")
# def random_zarr_dataset():
#     """Creates a random xarray Dataset.

#     Returns:
#         tuple: (dataset_path, expected_data)
#             - dataset_path: Path to the zarr store
#             - expected_data: The original xarray Dataset for comparison
#     """
#     # Coordinates of the random data
#     times = pd.date_range("2024-01-01", periods=100)
#     lats = np.linspace(-90, 90, 18)
#     lons = np.linspace(-180, 180, 36)

#     # Create random variables with different shapes
#     temp = np.random.randn(len(times), len(lats), len(lons))
#     precip = np.random.gamma(2, 0.5, size=(len(times), len(lats), len(lons)))

#     # Create the dataset
#     ds = xr.Dataset(
#         {
#             "temp": (
#                 ["time", "lat", "lon"],
#                 temp,
#                 {"units": "celsius", "long_name": "Surface Temperature"},
#             ),
#             "precip": (
#                 ["time", "lat", "lon"],
#                 precip,
#                 {"units": "mm/day", "long_name": "Daily Precipitation"},
#             ),
#         },
#         coords={
#             "time": times,
#             "lat": ("lat", lats, {"units": "degrees_north"}),
#             "lon": ("lon", lons, {"units": "degrees_east"}),
#         },
#         attrs={"description": "Test dataset with random weather data"},
#     )

#     yield ds


# # This test also collects miscellaneous statistics about performance, run with pytest -s to see these statistics being printed out
# @pytest.mark.asyncio
# async def test_write_read(
#     create_ipfs, random_zarr_dataset: xr.Dataset
# ):  # noqa for fixture which is imported above but then "redefined"
#     rpc_base_url, gateway_base_url = create_ipfs
#     test_ds = random_zarr_dataset
#     print("=== Writing this xarray Dataset to a Zarr v3 on IPFS ===")
#     print(test_ds)

#     async with KuboCAS(  # <-- own and auto-close session
#         rpc_base_url=rpc_base_url,
#         gateway_base_url=gateway_base_url,
#     ) as kubo_cas:
#         hamt = await HAMT.build(cas=kubo_cas, values_are_bytes=True)
#         zhs = ZarrHAMTStore(hamt)
#         assert zhs.supports_writes
#         start = time.perf_counter()
#         # Do an initial write along with an append which is a common xarray/zarr operation
#         test_ds.to_zarr(store=zhs)  # type: ignore
#         test_ds.to_zarr(store=zhs, mode="a", append_dim="time", zarr_format=3)  # type: ignore
#         end = time.perf_counter()
#         elapsed = end - start
#         print("=== Write Stats")
#         print(f"Total time in seconds: {elapsed:.2f}")
#         print("=== Root CID")
#         await hamt.make_read_only()
#         cid = hamt.root_node_id
#         print(cid)

#         print("=== Reading data back in and checking if identical")
#         hamt = await HAMT.build(
#             cas=kubo_cas, root_node_id=cid, values_are_bytes=True, read_only=True
#         )
#         start = time.perf_counter()
#         ipfs_ds: xr.Dataset
#         zhs = ZarrHAMTStore(hamt, read_only=True)
#         ipfs_ds = xr.open_zarr(store=zhs)
#         print(ipfs_ds)

#         # Check both halves, since each are an identical copy
#         ds1 = ipfs_ds.isel(time=slice(0, len(ipfs_ds.time) // 2))
#         ds2 = ipfs_ds.isel(time=slice(len(ipfs_ds.time) // 2, len(ipfs_ds.time)))
#         xr.testing.assert_identical(ds1, ds2)
#         xr.testing.assert_identical(test_ds, ds1)
#         xr.testing.assert_identical(test_ds, ds2)

#         end = time.perf_counter()
#         elapsed = end - start
#         print("=== Read Stats")
#         print(f"Total time in seconds: {elapsed:.2f}")

#         # Tests for code coverage's sake
#         assert await zhs.exists("zarr.json")
#         # __eq__
#         assert zhs == zhs
#         assert zhs != hamt
#         assert not zhs.supports_writes
#         assert not zhs.supports_partial_writes
#         assert not zhs.supports_deletes

#         hamt_keys = set()
#         async for k in zhs.hamt.keys():
#             hamt_keys.add(k)

#         zhs_keys: set[str] = set()
#         async for k in zhs.list():
#             zhs_keys.add(k)
#         assert hamt_keys == zhs_keys

#         zhs_keys: set[str] = set()
#         async for k in zhs.list():
#             zhs_keys.add(k)
#         assert hamt_keys == zhs_keys

#         zhs_keys: set[str] = set()
#         async for k in zhs.list_prefix(""):
#             zhs_keys.add(k)
#         assert hamt_keys == zhs_keys

#         with pytest.raises(NotImplementedError):
#             await zhs.set_partial_values([])

#         with pytest.raises(NotImplementedError):
#             await zhs.get_partial_values(
#                 zarr.core.buffer.default_buffer_prototype(), []
#             )

#         previous_zarr_json = await zhs.get(
#             "zarr.json", zarr.core.buffer.default_buffer_prototype()
#         )
#         assert previous_zarr_json is not None
#         # Setting a metadata file that should always exist should not change anything
#         await zhs.set_if_not_exists(
#             "zarr.json",
#             np.array([b"a"], dtype=np.bytes_),  # type: ignore
#         )  # type: ignore np.arrays, if dtype is bytes, is usable as a zarr buffer
#         zarr_json_now = await zhs.get(
#             "zarr.json", zarr.core.buffer.default_buffer_prototype()
#         )
#         assert zarr_json_now is not None
#         assert previous_zarr_json.to_bytes() == zarr_json_now.to_bytes()

#         # now remove that metadata file and then add it back
#         await zhs.hamt.enable_write()
#         zhs = ZarrHAMTStore(zhs.hamt, read_only=False)  # make a writable version
#         await zhs.delete("zarr.json")
#         # doing a duplicate delete should not result in an error
#         await zhs.delete("zarr.json")
#         zhs_keys: set[str] = set()
#         async for k in zhs.list():
#             zhs_keys.add(k)
#         assert hamt_keys != zhs_keys
#         assert "zarr.json" not in zhs_keys

#         await zhs.set_if_not_exists("zarr.json", previous_zarr_json)
#         zarr_json_now = await zhs.get(
#             "zarr.json", zarr.core.buffer.default_buffer_prototype()
#         )
#         assert zarr_json_now is not None
#         assert previous_zarr_json.to_bytes() == zarr_json_now.to_bytes()
