<p align="center">
<a href="https://dclimate.net/" target="_blank" rel="noopener noreferrer">
<img width="50%" src="https://user-images.githubusercontent.com/41392423/173133333-79ef15d0-6671-4be3-ac97-457344e9e958.svg" alt="dClimate logo">
</a>
</p>

[![codecov](https://codecov.io/gh/dClimate/py-hamt/graph/badge.svg?token=M6Y4D19Y38)](https://codecov.io/gh/dClimate/py-hamt)

# py-hamt
This is a python implementation of a HAMT, adapted from [rvagg's IAMap project written in JavaScript](https://github.com/rvagg/iamap).
Like IAMap, py-hamt abstracts over a backing storage layer which lets you store any arbitrary amount of data but returns its own ID, e.g. content-addressed systems.

Key differences from IAMap is that the py-hamt data structure is mutable and not asynchronous. But the key idea of abstracting over a value store is the same.

dClimate created this library to allow abstract to use IPFS as a [zarr](https://zarr.dev/) storage backend. To see this in action, see our [data ETLs](https://github.com/dClimate/etl-scripts).

# Usage
To install, since we do not publish this package to PyPI, add this library to your project directly from git. e.g.
```sh
pip install 'git+https://github.com/dClimate/py-hamt'
```

### Basic Writing/Reading from an in memory store
```python
from py_hamt import HAMT, DictStore

# Setup a HAMT with an in memory store 
in_memory_store = DictStore()
hamt = HAMT(store=in_memory_store)

# Set and get one value
hamt["foo"] = "bar"
assert "bar" == hamt["foo"]
assert len(hamt) == 1

# Set and get multiple values
hamt["foo"] = "bar1"
hamt["foo2"] = 2
assert 2 == hamt["foo2"]
assert len(hamt) == 2

# Iterate over keys
for key in hamt:
  print(key)
print (list(hamt)) # [foo, foo2], order depends on the hash function used

# Delete a value
del hamt["foo"]
assert len(hamt) == 1
```

### Reading a CID from IPFS 
```python
from py_hamt import HAMT, IPFSStore
from multiformats import CID

# Get the CID you wish to read whether from a blog post, a smart contract, or a friend
dataset_cid = "baf..."

# Use the multiformats library to decode the CID into an object
root_cid = CID.decode(dataset_cid)

# Create HAMT instance using the IPFSStore connecting to your locally 
# running IPFS Gateway from your local running IPFS Node, If you wish 
# you can change the default IPFS gateway
hamt = HAMT(store=IPFSStore(root_node_id=root_cid) # You can optionally pass your own gateway instead of defaults with the argument gateway_uri_stem="http://<IP>:<PORT>",

# Do something with the hamt key/values
... 
```

See the [code documentation](https://dclimate.github.io/py-hamt/py_hamt.html) for more on usage. Looking at the test files, namely `test_hamt.py` is also quite helpful. You can also see this library used in notebooks for data analysis here [dClimate Jupyter Notebooks](https://github.com/dClimate/jupyter-notebooks)

# Development Guide
## Setting Up
`py-hamt` uses [uv](https://docs.astral.sh/uv/) for project management. Make sure you install that first.
Once uv is installed, run
```sh
uv sync
```
to create the project virtual environment at `.venv` based on the lockfile `uv.lock`. Don't worry about activating this virtual environment to run tests or formatting and linting, uv will automatically take care of that.

## Run tests, formatting, linting
First, make sure you have the ipfs kubo daemon installed so that you can run the tests that utilize IPFS as a backing store. e.g. `ipfs daemon`. If needed, configure the test with your custom HTTP gateway and RPC API endpoints. Then run the script
```sh
bash run-checks.sh
```
This will run tests with code coverage information, then check formatting and linting.

We use `pytest` with 100% code coverage, and with test inputs that are both handwritten as well as generated by `hypothesis`. This allows us to try out millions of randomized inputs to create a more robust overall library.

Note that due to the randomized inputs, it is possible sometimes to get 99% or lower test coverage by pure chance. Rerun the tests to get back complete code coverage. If this happens on a GitHub action, try rerunning the action.

## CPU and Memory Profiling
We use python's native `cProfile` for running CPU profiles and snakeviz visualization the profile. We use `memray` for the memory profiling. We run the profile on the test suite, since the tests are supposed have complete code coverage anyhow.

Creating the CPU and memory profile requires manual activation of the virtual environment.
```sh
source .venv/bin/activate
python -m cProfile -o profile.prof -m pytest
python -m memray run -m pytest
```
The profile viewers can be directly invoked from uv.
```sh
uv run snakeviz .
```
```sh
uv run memray flamegraph <memray output> # e.g. <memray-output> = memray-pytest.12398.bin
```

## Generating and viewing documentation
`py-hamt` uses [pdoc](https://pdoc.dev/) for its ease of use. On pushes to main the docs folder will be deployed to the repository's GitHub pages, and PRs will contain preview deployments.

To see a documentation preview on your local machine, run
```sh
uv run pdoc py_hamt
```

## Managing dependencies
Use `uv add` and `uv remove`, e.g. `uv add numpy` or `uv add --dev pytest`. For more information please see the [uv documentation](https://docs.astral.sh/uv/guides/projects/).
